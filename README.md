## Gesture Based UI Project - Myo Armband | Unity
---
###### _Written by Conor McGrath & Danielis Joni≈°kis_
---  

#### Table of Contents
1. [Introduction](#introduction)
2. [Purpose of the application](#purpose-of-the-application)
3. [Gestures identified as appropriate for this application](#gestures-identified-as-appropriate-for-this-application)
4. [Hardware used in creating the application](#hardware-used-in-creating-the-application)
5. [Architecture for the solution](#architecture-for-the-solution)
6. [Conclusions & Recommendations](#conclusions--recommendations)
7. [References](#references)

---

#### Introduction
"Develop an application with a Natural User Interface.  There are a number of options available to you and this is an opportunity to combine a lot of technology that you have worked with over the past four years...At the very least, this should be a local implementation of the application using gestures to interact with it...You can reproduce a classic game or system using a gesture-based interface."

---

#### Purpose of the Application
The purpose of this application is to demonstrate how to use the Myo Armband to detect the electrical activity of muscles and control gestures and highly sensitive motion sensors detected by proprietary EMG muscle sensors to control the application to perform a series of operations. The Myo Armband has an Electromyogram (EMG) sensor that directly senses muscle activity and movement to read muscle activity in a refined manner.

This project is an example of the integration of Unity game engine with Myo. It is a good demonstration of the use of Myo gestures to perform a series of actions.

---

#### Gestures identified as approrpriate for this application
The Myo is an armband equipped with several Electromyography (EMG) sensors that can recognize hand gestures and the movement of the arms. Based on the electrical impulses generated by muscles, 8 EMG sensors are responsible to recognize and perform each gesture. 

![gestures](https://i.imgur.com/hMrofDQ.jpg)

When we came to an agreement to use the Myo armband for our application we first researched what gestures were available with this technology. We found out that the Myo armband recognizes 5 pre-set gestures out of the box.

We also looked into how user friendly the myo armband was for our application. As our lecturer kindly gave us a myo armband to work with we were able to figure out for ourselves just how user friendly the myo armband actually is. We found that the armband fits very comfortably on the users arm. No matter what size arm the user has it stretches and adjusts itself to the arm of the user.

_For this project, Myo provides the following intuitive hand movements/ gestures:_

![fist](https://i.imgur.com/KkFuZAf.png) The user makes a fist to shoot projectiles.

---
![wavein](https://i.imgur.com/FC9kZTM.png) The user waves in (Left) their wrist and the character ship moves left.

---
![waveout](https://i.imgur.com/WzYOSq9.png) The user waves out (Right) their wrist and the character ship moves right.

---
![spread](https://i.imgur.com/0jS9bVD.png) The user spreads out their fingers to ause the game and resume the game.

__The menu can also be interacted with using these features.__

---
#### Hardware used in creating the application
The hardware we chose for our application is the [Myo Armband](https://www.myo.com/). We choose to use the this hardware as it is a top of the range gesture control armband. The myo armband was also available to us in college, so that made it even more attractive to use.

List of hardware components:
1. Myo Armband
2. Myo sizing clips
3. Windows PC/Laptop or Mac( Windows Virtual Machine )

Another alternative hardware we could have used to build this application would be, for example a [Leap Motion](https://www.leapmotion.com/). The leap motion is a sensor device that supports hand and finger motions as input, but requires no hand contact or touching. This would have been good to use in our project also, but the myo armband was more practical for what we wanted to achieve.

---
#### Architecture for the solution
![architecture](https://i.imgur.com/RhU3QhU.png)
When deciding how to approach this project collectively we decided to choose to do the project using Microsoft's [Visual Studio](https://www.visualstudio.com/) IDE for [C#](https://en.wikipedia.org/wiki/C_Sharp_(programming_language)) and the [MYO Armband](https://www.myo.com/). We chose this as we had been working with both while in class and were familiar with coding in C#.

###### Class Diagram


---

##### Conclusions & Recommendations
This latest technology can help us to minimize the reliance on hardware and software for controlling robots and other devices. [Myo Armband](https://www.myo.com/) can easily realize the back screen control computer, in addition to playing computer games, browsing the web, controlling music entertainment and other entertainment activities, and even control the drone. Compared to [Kinect](https://www.xbox.com/en-US/xbox-one/accessories/kinect) and [Leap Motion](https://www.leapmotion.com/?lang=en), the advantage of [Myo](https://www.myo.com/) is that it is not restricted by the specific site and it is more natural to interact. The sensor on the [Myo armband](https://www.myo.com/) is internally equipped with electrodes so that the user can read out the bioelectric activity of the muscle when the user makes a telescopic gesture, thereby determining the intention of the wearer, and then sending the result of the computer processing to the receiver via the low-power **Bluetooth** control equipment. I think that [Siri's voice](https://www.apple.com/ios/siri/) interaction is not perfect for a large number of young people who are accustomed to keyboard input and textual ideograms.

---

##### References:
- [Myo Offical](https://www.myo.com/)
- [Kinect Offical](https://www.xbox.com/en-US/xbox-one/accessories/kinect)
- [Leap Motion Offical](https://www.leapmotion.com/?lang=en)
- [Siri's Voice Offical](https://www.apple.com/ios/siri/)
